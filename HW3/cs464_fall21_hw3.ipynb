{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs464_fall21_hw3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "bag-prediction",
      "language": "python",
      "name": "bag-prediction"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDSS2c_hXISc"
      },
      "source": [
        "**<h1><center>CS 464</center></h1>**\n",
        "**<h1><center>Introduction to Machine Learning</center></h1>**\n",
        "**<h1><center>Fall 2021</center></h1>**\n",
        "**<h1><center>Homework 3</center></h1>**\n",
        "<h4><center>Due: Jan 02, 2022 17:00 (GMT+3)</center></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5qurNR3XkKT"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "<ul>\n",
        "    <li>\n",
        "    This homework contains both written and programming questions about neural networks. You should implement programming questions on this notebook. Your plots should also be produced in this notebook. Each programming question has its own cell for your answer. You can implement your code directly in these cells, or you can call required functions which are defined in a different location for the given question.\n",
        "    </li>\n",
        "    <li>\n",
        "    For questions that you need to plot, your plot results have to be included in the cell output. For written questions, you may provide them either as comments in code cells or as seperate text cells. \n",
        "    </li>\n",
        "    <li>\n",
        "    You are <b>NOT ALLOWED</b> to use different libraries than given libraries in the code segments of this homework except for libraries inclueded in Python Standard Library (https://docs.python.org/3/library/).\n",
        "    </li>\n",
        "    <li>\n",
        "    You are <b>NOT ALLOWED</b> to use a different deep learning framework than PyTorch.\n",
        "    </li>\n",
        "    <li>\n",
        "    While submitting the homework file, please package notebook(\".ipynb\") and model (\".pth\") files as a gzipped TAR file or a ZIP file with the name cs464_hw3_section#_Firstname_Lastname. Please do not use any Turkish letters for any of your files including code files and model files. Upload your homework to Moodle.\n",
        "    </li>\n",
        "    <li>\n",
        "    This is an individual assignment for each student. That is, you are NOT allowed to share your work with your classmates.</li>\n",
        "    <li> \n",
        "    If you do not follow the submission routes, deadlines and specifications, it will lead to a significant grade deduction.\n",
        "    </li>\n",
        "    <li> \n",
        "    If you have any questions, please contact \"hakansivuk@gmail.com\".\n",
        "    </li>\n",
        "\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d7BOVodYiGe"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPlCx2GsYnHS"
      },
      "source": [
        "This homewrok is prepeared by using Google CoLab which already has required libraries. However, if you are using your own local Jupyter or any other Python notebook editor, you may use both anaconda or pip to install PyTorch to your own computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM4dpplWZBGS"
      },
      "source": [
        "### Anaconda Installation\n",
        "\n",
        "<ul>\n",
        "    <li>Download anaconda from https://www.anaconda.com/download</li>\n",
        "    <li>Follow the instructions provided in https://conda.io/docs/user-guide/install/index.html#regular-installation</li>\n",
        "</ul>\n",
        "\n",
        "#### Creation of Virtual Environment\n",
        "\n",
        "<ul>\n",
        "    <li>Create python3.7 virtual environment for your hw3 using follow command from the command line<br>\n",
        "        <i>> conda create -n HW3 python=3.7 anaconda</i></li>\n",
        "    <li>Activate your virtual environment<br>\n",
        "        <i>> source activate HW3</i></li>\n",
        "    <li>To install auxiliary libraries, replace the \"package_name\" in the following command and run it in activated \"hw3\" environment <br>\n",
        "        <i>> pip install \"package_name\"<i></li>\n",
        "     <li>When you create your virtual environment with \"anaconda\" metapackage, jupyter notebook should be installed. Try:<br>\n",
        "         <i>> jupyter notebook</i>\n",
        "</ul>\n",
        "\n",
        "\n",
        "#### Pytorch Installation with Anaconda\n",
        "\n",
        "You should install PyTorch to your virtual environment which is created for the hw3. Therefore, you should activate your homework virtual environment before to start PyTorch installation.\n",
        "<li>> source activate HW3</li>\n",
        "\n",
        "After you have activated the virtual environment, then use one of the following commands to install pytorch for CPU for your system. See https://pytorch.org/ for help.\n",
        "<ul>\n",
        "<li>For MacOS:<br>\n",
        "    <i>> conda install pytorch torchvision -c pytorch</i>\n",
        "</li>\n",
        "<li>For Linux:<br>\n",
        "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i>\n",
        "</li>\n",
        "<li>For Windows:<br>\n",
        "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i><br>\n",
        "</li>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVlcWIXEZZGO"
      },
      "source": [
        "###Pip3 Installation\n",
        "<ul>\n",
        "    <li>Download pip3 from https://pip.pypa.io/en/stable/installing/</li>\n",
        "    <li>If you are using Windows, you may need to add Python to your enviroment variables. You may use the following tutorial to install Python and pip.\n",
        "    https://phoenixnap.com/kb/how-to-install-python-3-windows</li>\n",
        "</ul>\n",
        "\n",
        "#### PyTorch Installation with Pip\n",
        "<ul>\n",
        "<li>For MacOS:<br>\n",
        "    <i>> pip3 install torch torchvision</i>\n",
        "</li>\n",
        "<li>For Linux:<br>\n",
        "    <i>> pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html</i>\n",
        "</li>\n",
        "<li>For Windows:<br>\n",
        "    <i>> pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html</i><br>\n",
        "</li>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0CsUtpmZmBk"
      },
      "source": [
        "##Question 1 [12 pts.]\n",
        "\n",
        "Answer the given questions with **at most a sentence**.\n",
        "\n",
        "  >a) Why do people use validation data?<br>\n",
        "\n",
        "  >b) What is the difference between mean squared error and mean absolute error?  <br>\n",
        "\n",
        "  >c) What is the main problem of using sigmoid as activation function in an artificial neural network (ANN)?<br>\n",
        "\n",
        ">d) What does it mean to overfit your data model?<br>\n",
        "\n",
        "  >e) Your input image size is 3x64x64. If you apply 3x3 convolution with input_channel=3, output_channel=6, padding=0, stride=2, what would be the size of the output?<br>\n",
        "\n",
        "  >f) In the previous question, how many trainable parameters are there? (you should also consider bias terms in addition to weights)<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnAVhRr4eTPo"
      },
      "source": [
        "##Question 2 [88 pts.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnaXgIskP6iY"
      },
      "source": [
        "Computer vision (CV) is the field of study that deals with how computers can gain high-level understanding from digital images or videos. Your task for this question is to classify scenes according to their contexts by using simple machine learning algorithms developed for CV problems on scene images.\n",
        "\n",
        "Your dataset consist of scene images from 4 contexts. Images of each context is stored under separate folders in the compressed file given to you.  The dataset has been processed in such a way that each class has approximately 2500 samples.\n",
        "\n",
        "Download the dataset from the following link:\n",
        "<br>\n",
        "https://drive.google.com/file/d/1l51t3aTY7B131fwq92ACI_b_D5Idq5In/view?usp=sharing\n",
        "<br>\n",
        "\n",
        "Libraries that are required in this question is given in the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z37KSYkSXHtz"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "# To Read Data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "# To Interpret results & obtain plots\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# You could add your own libraries form Python Standard Library in this cell. Any other external libraries are not allowed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkd-WIVwTQsn"
      },
      "source": [
        "### Data Loader [6 pts.]\n",
        "\n",
        "An important part of such a task is to implement your own data loader. In this homework, a partial loader is provided to you. This loader is going to be based on a base class named \"Dataset\", provided in PyTorch library. You need to complete the code below to create your custom \"SceneDataset\" class which will be able to load your dataset. Implement the functions whose proptotypes are given. Follow the TODO notes below. You have to divide the files into three sets as <b>train (70%)</b>, \n",
        "<b>validation (10%)</b> and **test (20%)** sets.  These non-overlapping splits, which are subsets of SceneDataset, should be retrieved using the \"get_dataset\" function.\n",
        "\n",
        "Hint: The dataset is not normalized and your results will heavily depend on your input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9fac_LpYdXE"
      },
      "source": [
        "class SceneDataset(Dataset):\n",
        "    # TODO:\n",
        "    # Define constructor for SceneDataset class\n",
        "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
        "    #def __init__(self, **kwargs): # you are free to change parameters\n",
        "        \n",
        "    '''This function should return sample count in the dataset'''\n",
        "    #def __len__(self):\n",
        "    #    return self.data.shape[0]\n",
        "\n",
        "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
        "    #def __getitem__(self, index):\n",
        "        #return _x, _y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE3b0p2IWrdo"
      },
      "source": [
        "def get_dataset(root):\n",
        "    # TODO: \n",
        "    # Read dataset files\n",
        "    # Construct training, validation and test sets\n",
        "    # Normalize datasets\n",
        "    \n",
        "    #return train_dataset, val_dataset, test_dataset#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFVQay-2Z7Ix"
      },
      "source": [
        "###Model Implementation [7 pts]\n",
        "\n",
        "Now implement your CNN. ConvNet class will represent your convolutional neural network. Implement 3 layers of convolution: \n",
        "<ul>\n",
        "    <li>(1) 4 filters with size of 3 x 3 with stride 1 and padding 1, (2) ReLU </li>\n",
        "    <li>(3) 8 filters with size of 3 x 3 with stride 1 and padding 1, (4) ReLU and (5) MaxPool 2 x 2 </li>\n",
        "    <li>(6) 16 filters with size of 3 x 3 with stride 1 and padding 1, (7) ReLU and (8) MaxPool 2 x 2 </li> \n",
        "</ul>\n",
        "\n",
        "As the classifier layer, you need to add only one linear layer at the end of the network. You need to choose the appropriate input and output neuron sizes and the activation function for the dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNnoeEy-aGA3"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    '''Define your neural network'''\n",
        "    def __init__(self, **kwargs): # you can add any additional parameters you want \n",
        "    # TODO:\n",
        "    # You should create your neural network here\n",
        "     \n",
        "    def forward(self, X): # you can add any additional parameters you want\n",
        "    # TODO:\n",
        "    # Forward propagation implementation should be here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4HT9fvm_jgX"
      },
      "source": [
        "###Stochastic Gradient Descent [25 pts.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJO7J4sf7YGZ"
      },
      "source": [
        "####Training with SGD [15 pts.]\n",
        "\n",
        "Train your model up to 300 epochs with properly processed inputs, i.e. call your \"get_dataset\" function. Use SGD as your optimizer. Tune your learning rate, weight decay. Do not add additional parameters to SGD. Save your best model as \"best_cnn_sgd.pth\". The best model should be selected based on validation dataset. You could use any measurement and/or metric to decide on the best model. However, you must explain your reasoning in your choice.\n",
        "\n",
        "During training, you need to plot two figures:\n",
        "1. training loss and validation loss vs. epoch\n",
        "2. training accuracy and validation accuracy vs. epoch <br>\n",
        "\n",
        "Name your axes and plots properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4VU9L1jCNCC"
      },
      "source": [
        "# HINT: note that your training time should not take more than 2 hours.\n",
        "\n",
        "# TODO:\n",
        "# Pick your hyper parameters\n",
        "max_epoch = 300\n",
        "# train_batch = \n",
        "# test_batch = \n",
        "# learning_rate = # try learning rate from the interval [1e-1, 1e-4]\n",
        "\n",
        "#use_gpu = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "# Create train dataset loader\n",
        "# Create validation dataset loader\n",
        "# Create test dataset loader\n",
        "# initialize your network\n",
        "model = ConvNet()\n",
        "\n",
        "# define your loss function\n",
        "    \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=5e-04) # you can play with weight_decay as well but do not add additional parameters\n",
        "    \n",
        "# start training\n",
        "# for each epoch calculate validation performance\n",
        "# save best model according to validation performance\n",
        "    \n",
        "# for epoch in range(max_epoch):\n",
        "#    model=model.train()\n",
        "#    iterate over training batches\n",
        "#    ...\n",
        "\n",
        "#    Validation\n",
        "#    model = model.eval()\n",
        "#    with torch.no_grad():\n",
        "#     iterate over validation batches\n",
        "#    if ???????:\n",
        "#       torch.save(model, best_path)\n",
        "\n",
        "# plot losses vs epoch \n",
        "# ...\n",
        "# plt.show()\n",
        "\n",
        "# plot accuracies vs epoch\n",
        "# ...\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGlkTe9e-VLO"
      },
      "source": [
        "####Test with SGD [10 pts.]\n",
        "\n",
        "Report the following for your best model on your test set which has not been seen by the model yet.\n",
        "1. A heatmap for confusion matrix\n",
        "2. Accuracy\n",
        "3. Macro Precision\n",
        "4. Macro Recall\n",
        "5. F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNc_qjAECN9V"
      },
      "source": [
        "# Test CNN\n",
        "# load best model\n",
        "# best_path = \"/content/drive/My Drive/.../best_cnn_sgd.pth\"\n",
        "# model = torch.load(best_path)\n",
        "\n",
        "# evaluate on test set\n",
        "# model = model.eval()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   iterate over test batches\n",
        "#   get confusion matrix\n",
        "#   calculate accuracy\n",
        "#   calculate precision\n",
        "#   calculate recall\n",
        "#   calculate F1 score\n",
        "\n",
        "# print metrics\n",
        "# print(\"Mean Loss:\", losses, \"\\nMean Acc:\", acc,\"\\nMean Macro Precision:\", pre, \"\\nMean Macro Recall:\", recall, \"\\nMean Macro F1 Score:\", f1) \n",
        "\n",
        "# plot confusion matrix\n",
        "# fig, ax = plt.subplots()\n",
        "# im = ax.imshow(conf_matrix)\n",
        "# We want to show all ticks...\n",
        "# ax.set_xticks(np.arange(15))\n",
        "# ax.set_yticks(np.arange(15))\n",
        "\n",
        "# fig.tight_layout()\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3L2zjEN_tZf"
      },
      "source": [
        "###Adam Optimizer [25 pts.]\n",
        "\n",
        "Adam is an adaptive learning rate optimization algorithm that has been designed specifically for training deep neural networks. It was presented by Diederik Kingma from OpenAI and Jimmy Ba from the University of Toronto in their 2015 ICLR paper (poster) titled “Adam: A Method for Stochastic Optimization“.\n",
        "\n",
        "Nowadays, most of machine learning frameworks, including tensorflow, Pytorch, and Keras, choose Adam as the default optimizer. In this question, you will experiment with it and try to understand why it replaced SGD as the default optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQ75RHP-Fdn"
      },
      "source": [
        "####Training with ADAM [15 pts.]\n",
        "\n",
        "Train your model up to 300 epochs with properly processed inputs, i.e. call your \"get_dataset\". This time use Adam Optimizer as your optimizer. Tune your learning rate, weight decay. Save your best model as \"best_cnn_adam.pth\". The best model should be selected based on validation dataset. You could use any measurement and/or metric to decide on the best model for each network. However, you must explain your reasoning in your choice.\n",
        "\n",
        "During training, you need to plot:\n",
        "1. training loss and validation loss vs. epoch\n",
        "2. training accuracy and validation accuracy vs. epoch <br>\n",
        "\n",
        "Name your axes and plots properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fr4WUpQCO5s"
      },
      "source": [
        "# HINT: note that your training time should not take more than 2 hours.\n",
        "\n",
        "# TODO:\n",
        "# Pick your hyper parameters\n",
        "max_epoch = 300\n",
        "# train_batch = \n",
        "# test_batch = \n",
        "# learning_rate = # try learning rate from the interval [1e-1, 1e-4]\n",
        "\n",
        "#use_gpu = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "# Create train dataset loader\n",
        "# Create validation dataset loader\n",
        "# Create test dataset loader\n",
        "# initialize your network\n",
        "model = ConvNet()\n",
        "\n",
        "# define your loss function\n",
        "    \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-04) # you can play with  weight_decay as well\n",
        "    \n",
        "# start training\n",
        "# for each epoch calculate validation performance\n",
        "# save best model according to validation performance\n",
        "    \n",
        "# for epoch in range(max_epoch):\n",
        "#    model=model.train()\n",
        "#    iterate over training batches\n",
        "#    ...\n",
        "\n",
        "#    Validation\n",
        "#    model = model.eval()\n",
        "#    with torch.no_grad():\n",
        "#     iterate over validation batches\n",
        "#    if ???????:\n",
        "#       torch.save(model, best_path)\n",
        "\n",
        "# plot losses vs epoch \n",
        "# ...\n",
        "# plt.show()\n",
        "\n",
        "# plot accuracies vs epoch\n",
        "# ...\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iozVeXZAXiH"
      },
      "source": [
        "####Test with ADAM [10 pts.]\n",
        "\n",
        "Report the following for your best model on your test set which has not been seen by the model yet.\n",
        "1. A heatmap for confusion matrix\n",
        "2. Accuracy\n",
        "3. Macro Precision\n",
        "4. Macro Recall\n",
        "5. F1 Score\n",
        "\n",
        "Then, discuss figures that you have plotted in the previous section, your test results and algorithm complexity with maximum 200 words. Compare two **optimizers**. Which one is more preferable? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t27l0hN9FqS"
      },
      "source": [
        "# Test CNN\n",
        "# load best model\n",
        "# best_path = \"/content/drive/My Drive/.../best_cnn_adam.pth\"\n",
        "# model = torch.load(best_path)\n",
        "\n",
        "# evaluate on test set\n",
        "# model = model.eval()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   iterate over test batches\n",
        "#   get confusion matrix\n",
        "#   calculate accuracy\n",
        "#   calculate precision\n",
        "#   calculate recall\n",
        "#   calculate F1 score\n",
        "\n",
        "# print metrics\n",
        "# print(\"Mean Loss:\", losses, \"\\nMean Acc:\", acc,\"\\nMean Macro Precision:\", pre, \"\\nMean Macro Recall:\", recall, \"\\nMean Macro F1 Score:\", f1) \n",
        "\n",
        "# plot confusion matrix\n",
        "# fig, ax = plt.subplots()\n",
        "# im = ax.imshow(conf_matrix)\n",
        "# We want to show all ticks...\n",
        "# ax.set_xticks(np.arange(15))\n",
        "# ax.set_yticks(np.arange(15))\n",
        "\n",
        "# fig.tight_layout()\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUsvHO7JUvK8"
      },
      "source": [
        "###Transfer Learning [25 pts.]\n",
        "\n",
        "Instead of training CNNs from scratch, you can use pretrained models and apply them to your task. Transfer learning is a machine learning technique where you can reuse a pretrained machine learning model as a starting point for your own task. In this question, you will experiment with it and try to understand why it is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SdoJEXuUvK9"
      },
      "source": [
        "####Training with Transfer Learning [15 pts.]\n",
        "\n",
        "Get pretrained ResNet18 model from torchvision.models and finetune your model up to 20 epochs with properly processed inputs, i.e. call your \"get_dataset\". This time use transfer learning. Tune your learning rate, weight decay. Save your best model as \"best_cnn_transfer.pth\". The best model should be selected based on validation dataset. You could use any measurement and/or metric to decide on the best model for each network. However, you must explain your reasoning in your choice.\n",
        "\n",
        "During training, you need to plot two figures:\n",
        "1. training loss and validation loss vs. epoch\n",
        "2. training accuracy and validation accuracy vs. epoch <br>\n",
        "\n",
        "Name your axes and plots properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q53czB57UvK9"
      },
      "source": [
        "# HINT: note that your training time should not take more than 2 hours.\n",
        "\n",
        "# TODO:\n",
        "# Pick your hyper parameters\n",
        "max_epoch = 20\n",
        "# train_batch = \n",
        "# test_batch = \n",
        "# learning_rate = # try learning rate from the interval [1e-1, 1e-4]\n",
        "\n",
        "#use_gpu = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "# Create train dataset loader\n",
        "# Create validation dataset loader\n",
        "# Create test dataset loader\n",
        "\n",
        "# initialize your network\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "# model.fc = replace its output layer with a linear layer (in_features, proper number according to your output classes)\n",
        "\n",
        "# define your loss function\n",
        "    \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, weight_decay = weight_decay) # you can play with momentum and weight_decay parameters as well\n",
        "    \n",
        "# start training\n",
        "# for each epoch calculate validation performance\n",
        "# save best model according to validation performance\n",
        "    \n",
        "# for epoch in range(max_epoch):\n",
        "#    model=model.train()\n",
        "#    iterate over training batches\n",
        "#    ...\n",
        "\n",
        "#    Validation\n",
        "#    model = model.eval()\n",
        "#    with torch.no_grad():\n",
        "#     iterate over validation batches\n",
        "#    if ???????:\n",
        "#       torch.save(model, best_path)\n",
        "\n",
        "# plot losses vs epoch \n",
        "# ...\n",
        "# plt.show()\n",
        "\n",
        "# plot accuracies vs epoch\n",
        "# ...\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu9HNrNMUvK_"
      },
      "source": [
        "####Test for Transfer Learning [10 pts.]\n",
        "\n",
        "Report the following for your best model on your test set which has not been seen by the model yet.\n",
        "1. A heatmap for confusion matrix\n",
        "2. Accuracy\n",
        "3. Macro Precision\n",
        "4. Macro Recall\n",
        "5. F1 Score\n",
        "\n",
        "Then, discuss figures that you have plotted in the previous section, your test results and algorithm complexity with maximum 200 words. Explain the advantages of using transfer learning. Is it better to reuse a pretrained model instead of training a model from scratch? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mUn9CdYUvLA"
      },
      "source": [
        "# Test CNN\n",
        "# load best model\n",
        "# best_path = \"/content/drive/My Drive/.../best_cnn_transfer.pth\"\n",
        "# model = torch.load(best_path)\n",
        "\n",
        "# evaluate on test set\n",
        "# model = model.eval()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   iterate over test batches\n",
        "#   get confusion matrix\n",
        "#   calculate accuracy\n",
        "#   calculate precision\n",
        "#   calculate recall\n",
        "#   calculate F1 score\n",
        "\n",
        "# print metrics\n",
        "# print(\"Mean Loss:\", losses, \"\\nMean Acc:\", acc,\"\\nMean Macro Precision:\", pre, \"\\nMean Macro Recall:\", recall, \"\\nMean Macro F1 Score:\", f1) \n",
        "\n",
        "# plot confusion matrix\n",
        "# fig, ax = plt.subplots()\n",
        "# im = ax.imshow(conf_matrix)\n",
        "# We want to show all ticks...\n",
        "# ax.set_xticks(np.arange(15))\n",
        "# ax.set_yticks(np.arange(15))\n",
        "\n",
        "# fig.tight_layout()\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF9y0N6jUvLC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}